{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Base colab code source for LDA model taken from: https://colab.research.google.com/github/MIND-Lab/OCTIS/blob/master/examples/OCTIS_LDA_training_only.ipynb\n",
        "\n",
        "BERTopic evaluation package: https://github.com/MaartenGr/BERTopic_evaluation"
      ],
      "metadata": {
        "id": "SOSZQuiQE24U"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zta66RCfDwpu",
        "outputId": "7c0d328c-47df-41c5-9eee-b93f182259a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting octis\n",
            "  Downloading octis-1.10.4-py2.py3-none-any.whl (129 kB)\n",
            "\u001b[K     |████████████████████████████████| 129 kB 7.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from octis) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.19.1 in /usr/local/lib/python3.8/dist-packages (from octis) (1.21.6)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.8/dist-packages (from octis) (3.4.3)\n",
            "Collecting sentence-transformers\n",
            "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
            "\u001b[K     |████████████████████████████████| 85 kB 2.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: flask in /usr/local/lib/python3.8/dist-packages (from octis) (1.1.4)\n",
            "Collecting tomotopy\n",
            "  Downloading tomotopy-0.12.3-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (16.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 16.5 MB 55.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.8/dist-packages (from octis) (3.2.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.8/dist-packages (from octis) (1.12.1+cu113)\n",
            "Collecting gensim>=4.0.0\n",
            "  Downloading gensim-4.2.0-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (24.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 24.1 MB 57 kB/s \n",
            "\u001b[?25hCollecting libsvm\n",
            "  Downloading libsvm-3.23.0.4.tar.gz (170 kB)\n",
            "\u001b[K     |████████████████████████████████| 170 kB 42.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from octis) (1.3.5)\n",
            "Collecting scikit-learn==0.24.2\n",
            "  Downloading scikit_learn-0.24.2-cp38-cp38-manylinux2010_x86_64.whl (24.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 24.9 MB 30.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: nltk in /usr/local/lib/python3.8/dist-packages (from octis) (3.7)\n",
            "Collecting scikit-optimize>=0.8.1\n",
            "  Downloading scikit_optimize-0.9.0-py2.py3-none-any.whl (100 kB)\n",
            "\u001b[K     |████████████████████████████████| 100 kB 5.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.8/dist-packages (from scikit-learn==0.24.2->octis) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn==0.24.2->octis) (3.1.0)\n",
            "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.8/dist-packages (from scikit-learn==0.24.2->octis) (1.7.3)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.8/dist-packages (from gensim>=4.0.0->octis) (5.2.1)\n",
            "Collecting pyaml>=16.9\n",
            "  Downloading pyaml-21.10.1-py2.py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.8/dist-packages (from pyaml>=16.9->scikit-optimize>=0.8.1->octis) (6.0)\n",
            "Requirement already satisfied: Werkzeug<2.0,>=0.15 in /usr/local/lib/python3.8/dist-packages (from flask->octis) (1.0.1)\n",
            "Requirement already satisfied: itsdangerous<2.0,>=0.24 in /usr/local/lib/python3.8/dist-packages (from flask->octis) (1.1.0)\n",
            "Requirement already satisfied: Jinja2<3.0,>=2.10.1 in /usr/local/lib/python3.8/dist-packages (from flask->octis) (2.11.3)\n",
            "Requirement already satisfied: click<8.0,>=5.1 in /usr/local/lib/python3.8/dist-packages (from flask->octis) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.8/dist-packages (from Jinja2<3.0,>=2.10.1->flask->octis) (2.0.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->octis) (1.4.4)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib->octis) (0.11.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->octis) (2.8.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->octis) (3.0.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.1->matplotlib->octis) (1.15.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from nltk->octis) (4.64.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.8/dist-packages (from nltk->octis) (2022.6.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas->octis) (2022.6)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->octis) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->octis) (2022.9.24)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->octis) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->octis) (2.10)\n",
            "Collecting transformers<5.0.0,>=4.6.0\n",
            "  Downloading transformers-4.25.1-py3-none-any.whl (5.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.8 MB 44.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torchvision in /usr/local/lib/python3.8/dist-packages (from sentence-transformers->octis) (0.13.1+cu113)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.97-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 49.7 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub>=0.4.0\n",
            "  Downloading huggingface_hub-0.11.1-py3-none-any.whl (182 kB)\n",
            "\u001b[K     |████████████████████████████████| 182 kB 52.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers->octis) (3.8.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers->octis) (4.1.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers->octis) (21.3)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.6 MB 38.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.8/dist-packages (from spacy->octis) (1.10.2)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.8/dist-packages (from spacy->octis) (2.0.8)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from spacy->octis) (1.0.3)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.8/dist-packages (from spacy->octis) (0.10.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /usr/local/lib/python3.8/dist-packages (from spacy->octis) (3.0.10)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.8/dist-packages (from spacy->octis) (8.1.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from spacy->octis) (57.4.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy->octis) (2.0.7)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.8/dist-packages (from spacy->octis) (1.0.9)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.8/dist-packages (from spacy->octis) (3.3.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy->octis) (3.0.8)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.8/dist-packages (from spacy->octis) (0.9.0)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.8/dist-packages (from spacy->octis) (2.4.5)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.8/dist-packages (from spacy->octis) (0.7.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy->octis) (0.0.3)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy->octis) (0.7.9)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.8/dist-packages (from torchvision->sentence-transformers->octis) (7.1.2)\n",
            "Building wheels for collected packages: libsvm, sentence-transformers\n",
            "  Building wheel for libsvm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for libsvm: filename=libsvm-3.23.0.4-cp38-cp38-linux_x86_64.whl size=233370 sha256=72f823f613b8593426af101cfc6b2abf10b1ad046517cd6d641dcd025d4dd12e\n",
            "  Stored in directory: /root/.cache/pip/wheels/41/00/e7/b328c0f5e4bbb9ac8ba5a72e56b1749be63dc1ab1c9321fd4e\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125938 sha256=a997f4b89ba5494adb5b41a7429f17d1e81a72b783e2a5be14285f8524ca4468\n",
            "  Stored in directory: /root/.cache/pip/wheels/5e/6f/8c/d88aec621f3f542d26fac0342bef5e693335d125f4e54aeffe\n",
            "Successfully built libsvm sentence-transformers\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers, sentencepiece, scikit-learn, pyaml, tomotopy, sentence-transformers, scikit-optimize, libsvm, gensim, octis\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.0.2\n",
            "    Uninstalling scikit-learn-1.0.2:\n",
            "      Successfully uninstalled scikit-learn-1.0.2\n",
            "  Attempting uninstall: gensim\n",
            "    Found existing installation: gensim 3.6.0\n",
            "    Uninstalling gensim-3.6.0:\n",
            "      Successfully uninstalled gensim-3.6.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "yellowbrick 1.5 requires scikit-learn>=1.0.0, but you have scikit-learn 0.24.2 which is incompatible.\u001b[0m\n",
            "Successfully installed gensim-4.2.0 huggingface-hub-0.11.1 libsvm-3.23.0.4 octis-1.10.4 pyaml-21.10.1 scikit-learn-0.24.2 scikit-optimize-0.9.0 sentence-transformers-2.2.2 sentencepiece-0.1.97 tokenizers-0.13.2 tomotopy-0.12.3 transformers-4.25.1\n",
            "\u001b[31mERROR: Directory '.[bertopic]' is not installable. Neither 'setup.py' nor 'pyproject.toml' found.\u001b[0m\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting bertopic==v0.9.4\n",
            "  Downloading bertopic-0.9.4-py2.py3-none-any.whl (57 kB)\n",
            "\u001b[K     |████████████████████████████████| 57 kB 3.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: plotly>=4.7.0 in /usr/local/lib/python3.8/dist-packages (from bertopic==v0.9.4) (5.5.0)\n",
            "Requirement already satisfied: pandas>=1.1.5 in /usr/local/lib/python3.8/dist-packages (from bertopic==v0.9.4) (1.3.5)\n",
            "Requirement already satisfied: scikit-learn>=0.22.2.post1 in /usr/local/lib/python3.8/dist-packages (from bertopic==v0.9.4) (0.24.2)\n",
            "Collecting hdbscan>=0.8.27\n",
            "  Downloading hdbscan-0.8.29.tar.gz (5.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.2 MB 16.8 MB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm>=4.41.1 in /usr/local/lib/python3.8/dist-packages (from bertopic==v0.9.4) (4.64.1)\n",
            "Collecting pyyaml<6.0\n",
            "  Downloading PyYAML-5.4.1-cp38-cp38-manylinux1_x86_64.whl (662 kB)\n",
            "\u001b[K     |████████████████████████████████| 662 kB 42.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.8/dist-packages (from bertopic==v0.9.4) (1.21.6)\n",
            "Requirement already satisfied: sentence-transformers>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from bertopic==v0.9.4) (2.2.2)\n",
            "Collecting umap-learn>=0.5.0\n",
            "  Downloading umap-learn-0.5.3.tar.gz (88 kB)\n",
            "\u001b[K     |████████████████████████████████| 88 kB 7.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: cython>=0.27 in /usr/local/lib/python3.8/dist-packages (from hdbscan>=0.8.27->bertopic==v0.9.4) (0.29.32)\n",
            "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.8/dist-packages (from hdbscan>=0.8.27->bertopic==v0.9.4) (1.2.0)\n",
            "Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.8/dist-packages (from hdbscan>=0.8.27->bertopic==v0.9.4) (1.7.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas>=1.1.5->bertopic==v0.9.4) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas>=1.1.5->bertopic==v0.9.4) (2022.6)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from plotly>=4.7.0->bertopic==v0.9.4) (1.15.0)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.8/dist-packages (from plotly>=4.7.0->bertopic==v0.9.4) (8.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.22.2.post1->bertopic==v0.9.4) (3.1.0)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.8/dist-packages (from sentence-transformers>=0.4.1->bertopic==v0.9.4) (4.25.1)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.8/dist-packages (from sentence-transformers>=0.4.1->bertopic==v0.9.4) (0.13.1+cu113)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.8/dist-packages (from sentence-transformers>=0.4.1->bertopic==v0.9.4) (0.1.97)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.8/dist-packages (from sentence-transformers>=0.4.1->bertopic==v0.9.4) (3.7)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from sentence-transformers>=0.4.1->bertopic==v0.9.4) (1.12.1+cu113)\n",
            "Requirement already satisfied: huggingface-hub>=0.4.0 in /usr/local/lib/python3.8/dist-packages (from sentence-transformers>=0.4.1->bertopic==v0.9.4) (0.11.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic==v0.9.4) (3.8.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic==v0.9.4) (2.23.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic==v0.9.4) (21.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic==v0.9.4) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.9->huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic==v0.9.4) (3.0.9)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.8/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic==v0.9.4) (0.13.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic==v0.9.4) (2022.6.2)\n",
            "Requirement already satisfied: numba>=0.49 in /usr/local/lib/python3.8/dist-packages (from umap-learn>=0.5.0->bertopic==v0.9.4) (0.56.4)\n",
            "Collecting pynndescent>=0.5\n",
            "  Downloading pynndescent-0.5.8.tar.gz (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 57.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from numba>=0.49->umap-learn>=0.5.0->bertopic==v0.9.4) (57.4.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.8/dist-packages (from numba>=0.49->umap-learn>=0.5.0->bertopic==v0.9.4) (4.13.0)\n",
            "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.8/dist-packages (from numba>=0.49->umap-learn>=0.5.0->bertopic==v0.9.4) (0.39.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata->numba>=0.49->umap-learn>=0.5.0->bertopic==v0.9.4) (3.10.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from nltk->sentence-transformers>=0.4.1->bertopic==v0.9.4) (7.1.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic==v0.9.4) (2022.9.24)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic==v0.9.4) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic==v0.9.4) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic==v0.9.4) (1.24.3)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.8/dist-packages (from torchvision->sentence-transformers>=0.4.1->bertopic==v0.9.4) (7.1.2)\n",
            "Building wheels for collected packages: hdbscan, umap-learn, pynndescent\n",
            "  Building wheel for hdbscan (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for hdbscan: filename=hdbscan-0.8.29-cp38-cp38-linux_x86_64.whl size=2700834 sha256=956ad9f356097d14971706d00bdafaeea90b05069d3a8ea8d40f89e9dbf05202\n",
            "  Stored in directory: /root/.cache/pip/wheels/76/06/48/527e038689c581cc9e519c73840efdc7473805149e55bd7ffd\n",
            "  Building wheel for umap-learn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for umap-learn: filename=umap_learn-0.5.3-py3-none-any.whl size=82829 sha256=46ca630056bab601d06106e065784d16879108827f126df01bf62b4b8fbdaac3\n",
            "  Stored in directory: /root/.cache/pip/wheels/a9/3a/67/06a8950e053725912e6a8c42c4a3a241410f6487b8402542ea\n",
            "  Building wheel for pynndescent (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pynndescent: filename=pynndescent-0.5.8-py3-none-any.whl size=55513 sha256=85e1e0ffe4b4f7b0af0cfb9fb213ad3dcb14dea17296aa23260d78f3abad7e6a\n",
            "  Stored in directory: /root/.cache/pip/wheels/1c/63/3a/29954bca1a27ba100ed8c27973a78cb71b43dc67aed62e80c3\n",
            "Successfully built hdbscan umap-learn pynndescent\n",
            "Installing collected packages: pyyaml, pynndescent, umap-learn, hdbscan, bertopic\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 6.0\n",
            "    Uninstalling PyYAML-6.0:\n",
            "      Successfully uninstalled PyYAML-6.0\n",
            "Successfully installed bertopic-0.9.4 hdbscan-0.8.29 pynndescent-0.5.8 pyyaml-5.4.1 umap-learn-0.5.3\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sklearn\n",
            "  Downloading sklearn-0.0.post1.tar.gz (3.6 kB)\n",
            "\u001b[33mWARNING: Discarding https://files.pythonhosted.org/packages/db/1e/af4e9cded5093a92e60d4ae7149a02c7427661b2db66c8ea4d34b17864a2/sklearn-0.0.post1.tar.gz#sha256=76b9ed1623775168657b86b5fe966d45752e5c87f528de6240c38923b94147c5 (from https://pypi.org/simple/sklearn/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\u001b[0m\n",
            "  Downloading sklearn-0.0.tar.gz (1.1 kB)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.8/dist-packages (from sklearn) (0.24.2)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->sklearn) (1.21.6)\n",
            "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->sklearn) (1.7.3)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->sklearn) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->sklearn) (1.2.0)\n",
            "Building wheels for collected packages: sklearn\n",
            "  Building wheel for sklearn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sklearn: filename=sklearn-0.0-py2.py3-none-any.whl size=1309 sha256=d9cada888737ef1c2cd51b3f6448d0d256f1e6884f81c134b130b86285f77f06\n",
            "  Stored in directory: /root/.cache/pip/wheels/22/0b/40/fd3f795caaa1fb4c6cb738bc1f56100be1e57da95849bfc897\n",
            "Successfully built sklearn\n",
            "Installing collected packages: sklearn\n",
            "Successfully installed sklearn-0.0\n"
          ]
        }
      ],
      "source": [
        "!pip install octis\n",
        "!pip install .[bertopic]\n",
        "!pip install bertopic==v0.9.4\n",
        "!pip install sklearn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from octis.models.LDA import LDA\n",
        "from octis.dataset.dataset import Dataset\n",
        "from octis.evaluation_metrics.diversity_metrics import TopicDiversity\n",
        "from octis.evaluation_metrics.coherence_metrics import Coherence\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "# from evaluation import Trainer\n",
        "# from data import DataLoader"
      ],
      "metadata": {
        "id": "9uXdu0C-EWcR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define dataset\n",
        "dataset = Dataset()\n",
        "dataset.fetch_dataset(\"20NewsGroup\")\n",
        "\n",
        "print(dataset.get_vocabulary())\n",
        "\n",
        "NUM_TOPICS = 20"
      ],
      "metadata": {
        "id": "fPMjMi0vEb8p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6749e280-117c-4739-a0c7-70529c937844"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['ability', 'absolute', 'absolutely', 'abuse', 'accept', 'acceptable', 'access', 'accident', 'accomplish', 'accord', 'account', 'accurate', 'achieve', 'act', 'action', 'active', 'activity', 'actual', 'add', 'addition', 'additional', 'address', 'adjust', 'administration', 'admit', 'adult', 'advance', 'advanced', 'advantage', 'advice', 'advocate', 'affect', 'afford', 'afraid', 'age', 'agency', 'agent', 'ago', 'agree', 'agreement', 'ahead', 'aid', 'aim', 'air', 'algorithm', 'alive', 'alt', 'alternative', 'amazing', 'amendment', 'american', 'amount', 'analysis', 'ancient', 'angel', 'animal', 'announce', 'announcement', 'annual', 'answer', 'anti', 'anymore', 'app', 'apparent', 'apparently', 'apple', 'application', 'apply', 'approach', 'approve', 'arab', 'archive', 'area', 'aren', 'argue', 'argument', 'arise', 'arm', 'armed', 'armenian', 'army', 'arrest', 'arrive', 'art', 'article', 'aspect', 'assault', 'assert', 'assist', 'associate', 'assume', 'assumption', 'assure', 'atheism', 'atheist', 'attach', 'attack', 'attempt', 'attention', 'attitude', 'attribute', 'audio', 'author', 'authority', 'auto', 'automatic', 'automatically', 'average', 'avoid', 'aware', 'baby', 'back', 'background', 'backup', 'bad', 'bag', 'balance', 'ball', 'ban', 'band', 'bank', 'bar', 'base', 'baseball', 'basic', 'basically', 'basis', 'bat', 'batf', 'battery', 'battle', 'bear', 'beat', 'begin', 'beginning', 'behavior', 'belief', 'belong', 'benefit', 'bet', 'bias', 'biblical', 'big', 'bike', 'bill', 'binary', 'bind', 'bio', 'bit', 'bite', 'black', 'blame', 'block', 'blood', 'blow', 'blue', 'board', 'body', 'bomb', 'book', 'boot', 'border', 'bother', 'bottom', 'box', 'boy', 'brain', 'brake', 'branch', 'brand', 'break', 'bring', 'broadcast', 'brother', 'btw', 'budget', 'bug', 'build', 'building', 'bullet', 'bunch', 'burn', 'bus', 'business', 'button', 'buy', 'byte', 'cable', 'call', 'camera', 'campaign', 'canadian', 'cap', 'capability', 'capable', 'capture', 'car', 'card', 'care', 'career', 'careful', 'carefully', 'carry', 'case', 'catch', 'category', 'catholic', 'cell', 'center', 'century', 'chain', 'challenge', 'chance', 'change', 'channel', 'chapter', 'character', 'charge', 'cheap', 'check', 'cheer', 'chemical', 'child', 'chip', 'choice', 'choose', 'christian', 'christianity', 'church', 'circle', 'circuit', 'circumstance', 'cite', 'citizen', 'city', 'civil', 'civilian', 'claim', 'class', 'clean', 'clear', 'client', 'clipper', 'clock', 'close', 'closely', 'clue', 'code', 'cold', 'collect', 'collection', 'college', 'color', 'combination', 'combine', 'command', 'comment', 'commercial', 'commit', 'common', 'communication', 'community', 'company', 'compare', 'comparison', 'compatible', 'compile', 'complain', 'complaint', 'complete', 'completely', 'complex', 'component', 'compound', 'compress', 'compression', 'compromise', 'computer', 'concept', 'concern', 'concerned', 'conclude', 'conclusion', 'condemn', 'condition', 'conduct', 'conference', 'configuration', 'confirm', 'conflict', 'confuse', 'connect', 'connection', 'connector', 'consequence', 'consideration', 'consist', 'consistent', 'constant', 'constitution', 'contact', 'content', 'context', 'continue', 'contract', 'contrary', 'contribute', 'control', 'controller', 'conversation', 'conversion', 'convert', 'convince', 'cool', 'cop', 'copy', 'copyright', 'core', 'corner', 'correct', 'correction', 'correctly', 'cost', 'couldn', 'count', 'country', 'couple', 'court', 'cover', 'coverage', 'cpu', 'crack', 'crap', 'crash', 'create', 'creation', 'credit', 'crime', 'criminal', 'critical', 'cross', 'cult', 'culture', 'curious', 'current', 'customer', 'cut', 'cycle', 'daily', 'damage', 'damn', 'danger', 'dangerous', 'dark', 'data', 'database', 'date', 'datum', 'day', 'dead', 'deal', 'dealer', 'death', 'debate', 'decade', 'decent', 'decide', 'decision', 'declare', 'deep', 'default', 'defend', 'defense', 'define', 'definition', 'degree', 'delay', 'delete', 'deliver', 'demand', 'demonstrate', 'deny', 'department', 'depend', 'depth', 'derive', 'describe', 'description', 'deserve', 'design', 'desire', 'destroy', 'detail', 'detect', 'determine', 'develop', 'development', 'device', 'devil', 'didn', 'die', 'differ', 'difference', 'difficult', 'difficulty', 'digital', 'direct', 'direction', 'directly', 'directory', 'disagree', 'disclaimer', 'discover', 'discuss', 'discussion', 'disease', 'disk', 'display', 'distance', 'distribute', 'distribution', 'divide', 'division', 'doctor', 'doctrine', 'document', 'documentation', 'doesn', 'dog', 'dollar', 'domain', 'don', 'door', 'double', 'doubt', 'download', 'dozen', 'draft', 'drag', 'draw', 'dream', 'drink', 'drive', 'driver', 'drop', 'drug', 'dry', 'due', 'duty', 'earlier', 'early', 'earth', 'easily', 'easy', 'eat', 'economic', 'edge', 'edit', 'edition', 'editor', 'education', 'effect', 'effective', 'effectively', 'effort', 'electronic', 'element', 'eliminate', 'email', 'enable', 'encounter', 'encourage', 'encrypt', 'encryption', 'end', 'energy', 'enforcement', 'engage', 'engine', 'engineer', 'engineering', 'enjoy', 'ensure', 'enter', 'entire', 'entry', 'environment', 'equal', 'equally', 'equipment', 'equivalent', 'error', 'escape', 'escrow', 'essentially', 'establish', 'estimate', 'eternal', 'european', 'event', 'eventually', 'evidence', 'evil', 'exact', 'examine', 'excellent', 'exception', 'exchange', 'excuse', 'execute', 'exercise', 'exist', 'existence', 'exit', 'expand', 'expansion', 'expect', 'expensive', 'experience', 'experiment', 'expert', 'explain', 'explanation', 'export', 'expose', 'express', 'extend', 'extension', 'extent', 'external', 'extra', 'extreme', 'extremely', 'eye', 'face', 'facility', 'fact', 'factor', 'fail', 'failure', 'fair', 'fairly', 'faith', 'fall', 'false', 'familiar', 'family', 'fan', 'faq', 'fast', 'faster', 'father', 'fault', 'favor', 'favorite', 'fax', 'fear', 'feature', 'federal', 'fee', 'feed', 'feel', 'feeling', 'fellow', 'field', 'fight', 'figure', 'file', 'fill', 'film', 'filter', 'final', 'finally', 'find', 'fine', 'finger', 'finish', 'fire', 'firearm', 'firm', 'fit', 'fix', 'flame', 'flat', 'flight', 'floor', 'floppy', 'flow', 'fly', 'focus', 'folk', 'follow', 'follower', 'font', 'food', 'fool', 'foot', 'force', 'foreign', 'forget', 'form', 'format', 'forward', 'fourth', 'frame', 'free', 'freedom', 'frequency', 'frequently', 'friend', 'front', 'fuel', 'full', 'fully', 'fun', 'function', 'fund', 'fundamental', 'funny', 'future', 'gain', 'game', 'gas', 'gather', 'gay', 'gear', 'general', 'generally', 'generate', 'generation', 'genocide', 'german', 'give', 'glad', 'goal', 'god', 'good', 'gospel', 'government', 'grab', 'grant', 'graphic', 'great', 'greatly', 'greek', 'green', 'ground', 'group', 'grow', 'guarantee', 'guess', 'guide', 'guilty', 'gun', 'guy', 'half', 'hand', 'handle', 'hang', 'happen', 'happy', 'hard', 'hardware', 'harm', 'hate', 'haven', 'head', 'header', 'health', 'hear', 'heart', 'heat', 'heavy', 'height', 'hell', 'helpful', 'hide', 'high', 'highly', 'hint', 'historical', 'history', 'hit', 'hockey', 'hold', 'hole', 'holy', 'home', 'homosexual', 'homosexuality', 'honest', 'hook', 'hope', 'hospital', 'host', 'hot', 'hour', 'house', 'huge', 'human', 'hundred', 'hurt', 'ice', 'ide', 'idea', 'identical', 'identify', 'ignorance', 'ignore', 'illegal', 'image', 'imagine', 'imho', 'immediately', 'impact', 'implement', 'implementation', 'imply', 'importance', 'important', 'impossible', 'impression', 'improve', 'improvement', 'inch', 'incident', 'include', 'increase', 'independent', 'index', 'individual', 'industry', 'influence', 'info', 'inform', 'information', 'initial', 'injury', 'innocent', 'input', 'insert', 'inside', 'insist', 'instal', 'install', 'installation', 'instance', 'instruction', 'instrument', 'insurance', 'intelligence', 'intend', 'intent', 'interest', 'interested', 'interesting', 'interface', 'internal', 'international', 'internet', 'interpret', 'interpretation', 'interview', 'introduce', 'introduction', 'investigation', 'involve', 'israeli', 'issue', 'item', 'jewish', 'job', 'join', 'joke', 'judge', 'jump', 'justice', 'justify', 'key', 'keyboard', 'kick', 'kid', 'kill', 'kind', 'king', 'kit', 'knock', 'knowledge', 'lab', 'label', 'lack', 'land', 'language', 'large', 'laser', 'late', 'launch', 'law', 'lawyer', 'lay', 'lead', 'leader', 'league', 'learn', 'leave', 'left', 'leg', 'legal', 'legitimate', 'length', 'letter', 'level', 'liberal', 'liberty', 'library', 'license', 'lie', 'life', 'light', 'limit', 'limited', 'line', 'link', 'list', 'listen', 'literature', 'live', 'load', 'local', 'locate', 'location', 'lock', 'logic', 'logical', 'long', 'longer', 'lose', 'loss', 'lot', 'love', 'low', 'luck', 'lucky', 'machine', 'magazine', 'mail', 'mailing', 'main', 'maintain', 'major', 'majority', 'make', 'male', 'man', 'manage', 'management', 'manager', 'manner', 'manual', 'manufacture', 'manufacturer', 'map', 'mark', 'market', 'mass', 'massacre', 'master', 'match', 'material', 'matter', 'maximum', 'meaning', 'measure', 'mechanism', 'medical', 'medicine', 'medium', 'meet', 'meeting', 'meg', 'member', 'memory', 'mention', 'menu', 'mess', 'message', 'metal', 'method', 'mhz', 'middle', 'mile', 'military', 'million', 'mind', 'mine', 'minimum', 'minor', 'minority', 'minute', 'mirror', 'miss', 'mission', 'mistake', 'mix', 'mode', 'model', 'modem', 'modern', 'modify', 'moment', 'money', 'monitor', 'month', 'moon', 'moral', 'morality', 'morning', 'mother', 'motherboard', 'motif', 'motor', 'motorcycle', 'mount', 'mouse', 'mouth', 'move', 'movement', 'movie', 'multi', 'multiple', 'murder', 'music', 'muslim', 'nation', 'national', 'natural', 'nature', 'necessarily', 'negative', 'neighbor', 'net', 'network', 'news', 'newsgroup', 'newspaper', 'nice', 'night', 'noise', 'normal', 'north', 'note', 'notice', 'notion', 'number', 'numerous', 'object', 'objective', 'observation', 'observe', 'obtain', 'obvious', 'occupy', 'occur', 'odd', 'offer', 'office', 'officer', 'official', 'oil', 'open', 'operate', 'operation', 'opinion', 'opportunity', 'oppose', 'opposite', 'option', 'orbit', 'order', 'organization', 'organize', 'origin', 'original', 'originally', 'output', 'owner', 'pack', 'package', 'page', 'pain', 'paint', 'pair', 'panel', 'paper', 'paragraph', 'parallel', 'parent', 'park', 'part', 'participate', 'party', 'pass', 'passage', 'past', 'patch', 'path', 'patient', 'pattern', 'pay', 'peace', 'pen', 'penalty', 'people', 'percent', 'percentage', 'perfect', 'perfectly', 'perform', 'performance', 'period', 'permission', 'permit', 'person', 'personal', 'personally', 'perspective', 'philosophy', 'phone', 'phrase', 'physical', 'pick', 'picture', 'piece', 'pin', 'pitch', 'pitcher', 'place', 'plain', 'plan', 'plane', 'planet', 'plastic', 'plate', 'platform', 'play', 'player', 'playoff', 'plenty', 'plug', 'point', 'pointer', 'police', 'policy', 'politic', 'political', 'poor', 'pop', 'popular', 'population', 'port', 'portion', 'position', 'positive', 'possibility', 'possibly', 'post', 'poster', 'posting', 'potential', 'pound', 'power', 'powerful', 'practical', 'practice', 'pray', 'pre', 'precisely', 'predict', 'prefer', 'prepare', 'presence', 'present', 'preserve', 'president', 'press', 'pressure', 'pretty', 'prevent', 'previous', 'previously', 'price', 'primarily', 'primary', 'principle', 'print', 'printer', 'prior', 'privacy', 'private', 'pro', 'problem', 'procedure', 'proceed', 'process', 'processing', 'processor', 'produce', 'product', 'production', 'professional', 'profit', 'program', 'programming', 'progress', 'project', 'promise', 'promote', 'proof', 'proper', 'properly', 'property', 'proposal', 'propose', 'protect', 'protection', 'prove', 'provide', 'pub', 'public', 'publication', 'publish', 'pull', 'punishment', 'purchase', 'purpose', 'push', 'put', 'quality', 'question', 'quick', 'quickly', 'quit', 'quote', 'race', 'radio', 'raise', 'ram', 'random', 'range', 'ranger', 'rape', 'rate', 'reach', 'reaction', 'read', 'reader', 'reading', 'ready', 'real', 'reality', 'realize', 'rear', 'reason', 'reasonable', 'recall', 'receive', 'recent', 'recently', 'recognize', 'recommend', 'record', 'red', 'reduce', 'refer', 'reference', 'reflect', 'refuse', 'regard', 'region', 'register', 'registration', 'regular', 'regularly', 'reject', 'relate', 'related', 'relation', 'relationship', 'relative', 'release', 'relevant', 'reliable', 'religion', 'religious', 'rely', 'remain', 'remark', 'remember', 'remind', 'remote', 'remove', 'render', 'repair', 'repeat', 'replace', 'replacement', 'reply', 'report', 'represent', 'representative', 'request', 'require', 'requirement', 'research', 'reserve', 'resistance', 'resolution', 'resource', 'respect', 'respond', 'response', 'responsibility', 'responsible', 'rest', 'restrict', 'restriction', 'result', 'return', 'reveal', 'reverse', 'review', 'rich', 'rid', 'ride', 'ring', 'rise', 'risk', 'road', 'rock', 'rocket', 'role', 'roll', 'rom', 'room', 'root', 'round', 'routine', 'rule', 'run', 'russian', 'safe', 'safety', 'sale', 'sample', 'satellite', 'save', 'scale', 'scan', 'schedule', 'scheme', 'school', 'science', 'scientific', 'scientist', 'score', 'screen', 'screw', 'scripture', 'scsi', 'seal', 'search', 'season', 'seat', 'secret', 'section', 'secure', 'security', 'seek', 'select', 'selection', 'sell', 'send', 'sense', 'sentence', 'separate', 'sequence', 'serial', 'series', 'serve', 'server', 'service', 'session', 'set', 'setting', 'settle', 'setup', 'sex', 'sexual', 'shape', 'share', 'shareware', 'sheet', 'shell', 'shift', 'ship', 'shipping', 'shit', 'shoot', 'shop', 'short', 'shot', 'shouldn', 'show', 'shut', 'shuttle', 'sick', 'side', 'sign', 'signal', 'signature', 'significant', 'significantly', 'silly', 'similar', 'similarly', 'simple', 'simply', 'sin', 'single', 'sit', 'site', 'situation', 'size', 'skill', 'sleep', 'slightly', 'slot', 'slow', 'small', 'smoke', 'social', 'society', 'software', 'solar', 'soldier', 'solid', 'solution', 'solve', 'son', 'sort', 'soul', 'sound', 'source', 'soviet', 'space', 'spare', 'speak', 'speaker', 'spec', 'special', 'specific', 'specifically', 'speech', 'speed', 'spend', 'split', 'sport', 'spot', 'spread', 'spring', 'stable', 'staff', 'stage', 'stand', 'standard', 'star', 'start', 'stat', 'state', 'statement', 'station', 'statistic', 'status', 'stay', 'steal', 'step', 'stick', 'stock', 'stone', 'stop', 'storage', 'store', 'story', 'straight', 'strange', 'strategy', 'street', 'strike', 'string', 'strong', 'strongly', 'structure', 'struggle', 'student', 'study', 'stuff', 'stupid', 'style', 'subject', 'submit', 'success', 'successful', 'suck', 'suddenly', 'suffer', 'sufficient', 'suggest', 'suggestion', 'suit', 'summary', 'summer', 'sun', 'super', 'supply', 'support', 'suppose', 'supposedly', 'surely', 'surface', 'surprise', 'surprised', 'surrender', 'surround', 'survey', 'survive', 'suspect', 'switch', 'system', 'table', 'talk', 'tank', 'tape', 'target', 'task', 'tax', 'taxis', 'teach', 'teaching', 'team', 'tear', 'tech', 'technical', 'technique', 'technology', 'telephone', 'temperature', 'tend', 'term', 'terminal', 'territory', 'terrorist', 'test', 'text', 'theory', 'thing', 'thinking', 'thought', 'thousand', 'thread', 'threat', 'threaten', 'throw', 'ticket', 'tie', 'time', 'tip', 'tire', 'title', 'today', 'tool', 'top', 'topic', 'total', 'totally', 'touch', 'tough', 'town', 'trace', 'track', 'trade', 'tradition', 'traditional', 'traffic', 'train', 'training', 'transfer', 'translate', 'translation', 'transmission', 'transmit', 'travel', 'treat', 'treatment', 'tree', 'trial', 'trick', 'trip', 'troop', 'trouble', 'truck', 'true', 'trust', 'truth', 'turkish', 'turn', 'type', 'typical', 'unable', 'understand', 'understanding', 'unique', 'unit', 'universe', 'university', 'unix', 'unknown', 'update', 'upgrade', 'useless', 'usenet', 'user', 'usual', 'utility', 'valid', 'variable', 'variety', 'vary', 'vehicle', 'vendor', 'verify', 'verse', 'version', 'vga', 'vice', 'victim', 'video', 'view', 'village', 'violate', 'violation', 'violence', 'violent', 'virtual', 'virtually', 'visible', 'vision', 'visit', 'voice', 'voltage', 'volume', 'vote', 'wait', 'walk', 'wall', 'war', 'warn', 'warning', 'warrant', 'warranty', 'wasn', 'waste', 'watch', 'water', 'wave', 'weak', 'weapon', 'wear', 'week', 'weekend', 'weight', 'western', 'wheel', 'white', 'wide', 'widely', 'widget', 'wife', 'win', 'wind', 'window', 'wing', 'wire', 'witness', 'woman', 'wonderful', 'word', 'work', 'workstation', 'world', 'worry', 'worship', 'worth', 'wouldn', 'wound', 'write', 'writer', 'writing', 'wrong', 'year', 'yesterday', 'young', 'zone']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Model\n",
        "model = LDA(num_topics=NUM_TOPICS, alpha=0.1)"
      ],
      "metadata": {
        "id": "v0rOVshFEe7V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model using default partitioning choice \n",
        "output = model.train_model(dataset)\n",
        "\n",
        "print(*list(output.keys()), sep=\"\\n\") # Print the output identifiers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ijCifdE1Ejq7",
        "outputId": "86f31fb7-c97f-480b-c8b4-425de270c56a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:gensim.models.ldamodel:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "topic-word-matrix\n",
            "topics\n",
            "topic-document-matrix\n",
            "test-topic-document-matrix\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for t in output['topics'][:5]:\n",
        "  print(\" \".join(t))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JWvFkvp_Eswn",
        "outputId": "dfa9e70c-2c8e-467c-92b8-9a191c3f4157"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "game team win year play good player make season time\n",
            "car buy money problem company pay year work sell good\n",
            "chip card mode bit run time communication bank work system\n",
            "make thing good time point problem people find light back\n",
            "book jewish patient arab disease page information title greek program\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize metric\n",
        "npmi = Coherence(texts=dataset.get_corpus(), topk=10, measure='c_npmi')\n",
        "umass = Coherence(texts=dataset.get_corpus(), topk=10, measure='u_mass')\n",
        "cv = Coherence(texts=dataset.get_corpus(), topk=10, measure='c_v')\n",
        "uci = Coherence(texts=dataset.get_corpus(), topk=10, measure='c_uci')"
      ],
      "metadata": {
        "id": "e-g1iGt8Evkv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize metric\n",
        "topic_diversity = TopicDiversity(topk=10)"
      ],
      "metadata": {
        "id": "wWGzTDX_Eylz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieve metrics score\n",
        "topic_diversity_score = topic_diversity.score(output)\n",
        "print(\"Topic diversity: \"+str(topic_diversity_score))\n",
        "\n",
        "npmi_score = npmi.score(output)\n",
        "print(\"Coherence: \"+str(npmi_score))\n",
        "umass_score = umass.score(output)\n",
        "print(\"Coherence: \"+str(umass_score))\n",
        "cv_score = cv.score(output)\n",
        "print(\"Coherence: \"+str(cv_score))\n",
        "uci_score = uci.score(output)\n",
        "print(\"Coherence: \"+str(uci_score))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LnnX6ma4E0M3",
        "outputId": "23c99dc4-5f72-4249-f3bd-bea48d06abdc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Topic diversity: 0.74\n",
            "Coherence: 0.04804667849438671\n",
            "Coherence: -2.2445109826639307\n",
            "Coherence: 0.519133103675679\n",
            "Coherence: 0.017321758979920034\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Prepare data\n",
        "dataset, custom = \"20NewsGroup\", False\n",
        "data_loader = DataLoader(dataset)\n",
        "_, timestamps = data_loader.load_docs()\n",
        "data = data_loader.load_octis(custom)\n",
        "data = [\" \".join(words) for words in data.get_corpus()]\n",
        "\n",
        "# Extract embeddings\n",
        "model = SentenceTransformer(\"all-mpnet-base-v2\")\n",
        "embeddings = model.encode(data, show_progress_bar=True)"
      ],
      "metadata": {
        "id": "-3MP7nRNLfFQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "with open('embeddings.pickle', 'wb') as handle:\n",
        "    pickle.dump((dataset, custom, embeddings), handle, protocol=pickle.HIGHEST_PROTOCOL)"
      ],
      "metadata": {
        "id": "iDpwkzonsh0j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "with open('embeddings.pickle', 'rb') as handle:\n",
        "    dataset, custom, embeddings = pickle.load(handle)"
      ],
      "metadata": {
        "id": "w__jNSyosyme"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from evaluation import Trainer\n",
        "print(str(dataset))\n",
        "params = {\n",
        "    \"embedding_model\": \"all-mpnet-base-v2\",\n",
        "    \"nr_topics\": [NUM_TOPICS for i in range(5)],\n",
        "    \"min_topic_size\": 15,\n",
        "    \"diversity\": None,\n",
        "    \"verbose\": True\n",
        "}\n",
        "\n",
        "trainer = Trainer(dataset=dataset,\n",
        "                  model_name=\"BERTopic\",\n",
        "                  params=params,\n",
        "                  bt_embeddings=embeddings,\n",
        "                  custom_dataset=custom,)\n",
        "                  # verbose=True)\n",
        "results = trainer.train(save=f\"BERTopic_news_{1}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 989
        },
        "id": "HwPTy_VgLkbO",
        "outputId": "d9970277-a381-44aa-edfb-6144aa75708e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20NewsGroup\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-12-05 00:41:07,738 - BERTopic - Reduced dimensionality with UMAP\n",
            "2022-12-05 00:41:08,725 - BERTopic - Clustered UMAP embeddings with HDBSCAN\n",
            "2022-12-05 00:41:27,239 - BERTopic - Reduced number of topics from 85 to 21\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results123\n",
            "============\n",
            "npmi: 0.18182166234282388\n",
            "umass: -1.7965300794488983\n",
            "cv: 0.7425894106367811\n",
            "uci: 1.4153266737303767\n",
            "diversity: 0.9\n",
            " \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-12-05 00:42:25,919 - BERTopic - Reduced dimensionality with UMAP\n",
            "2022-12-05 00:42:26,854 - BERTopic - Clustered UMAP embeddings with HDBSCAN\n",
            "2022-12-05 00:42:46,870 - BERTopic - Reduced number of topics from 86 to 21\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results123\n",
            "============\n",
            "npmi: 0.18374540244260112\n",
            "umass: -1.8825536021191254\n",
            "cv: 0.7416305115371481\n",
            "uci: 1.4124947917432362\n",
            "diversity: 0.88\n",
            " \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-12-05 00:43:40,050 - BERTopic - Reduced dimensionality with UMAP\n",
            "2022-12-05 00:43:41,011 - BERTopic - Clustered UMAP embeddings with HDBSCAN\n",
            "2022-12-05 00:44:00,310 - BERTopic - Reduced number of topics from 85 to 21\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results123\n",
            "============\n",
            "npmi: 0.18598519827807083\n",
            "umass: -1.8521944391744114\n",
            "cv: 0.7400174770881423\n",
            "uci: 1.462846612425816\n",
            "diversity: 0.905\n",
            " \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-12-05 00:44:56,659 - BERTopic - Reduced dimensionality with UMAP\n",
            "2022-12-05 00:44:57,577 - BERTopic - Clustered UMAP embeddings with HDBSCAN\n",
            "2022-12-05 00:45:16,782 - BERTopic - Reduced number of topics from 81 to 21\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results123\n",
            "============\n",
            "npmi: 0.1855918150331591\n",
            "umass: -1.8944230857100421\n",
            "cv: 0.7527964370622736\n",
            "uci: 1.44410616962557\n",
            "diversity: 0.905\n",
            " \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-12-05 00:46:12,068 - BERTopic - Reduced dimensionality with UMAP\n",
            "2022-12-05 00:46:12,980 - BERTopic - Clustered UMAP embeddings with HDBSCAN\n",
            "2022-12-05 00:46:32,984 - BERTopic - Reduced number of topics from 89 to 21\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results123\n",
            "============\n",
            "npmi: 0.17778786299342791\n",
            "umass: -1.8798098878148835\n",
            "cv: 0.7342561860382728\n",
            "uci: 1.3821971649500149\n",
            "diversity: 0.92\n",
            " \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_f68a9fa6-0546-45ca-8c6c-be9fe1ce160f\", \"BERTopic_news_1.json\", 2073)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}